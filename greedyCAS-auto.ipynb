{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c858beb-2fea-49a9-bd52-f10f6a874330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster.supervised import contingency_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from nltk.metrics.segmentation import pk, windowdiff\n",
    "from segeval.window.windowdiff import window_diff\n",
    "from scipy import sparse as sp\n",
    "from harvesttext import HarvestText\n",
    "\n",
    "import scipy\n",
    "import random\n",
    "import nltk\n",
    "import numpy as np\n",
    "from functools import lru_cache\n",
    "from nltk.tokenize import  RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from keybert import KeyBERT\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import ast\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "from sqlitedict import SqliteDict\n",
    "from sentence_splitter import SentenceSplitter, split_text_into_sentences\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import spacy\n",
    "from scipy import stats\n",
    "import os\n",
    "import pickle\n",
    "from numba import jit\n",
    "import itertools\n",
    "import concurrent.futures\n",
    "\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78168bee-1c07-49bb-b9b0-0498ed80c76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef5a73a-9ce1-4c9d-a880-7860a743f291",
   "metadata": {},
   "source": [
    "#### Define abstract processing regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c208532-fc24-44bd-89f9-016e0241378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_background = r'BACKGROUND(.*?)METHODS'\n",
    "regex_method = r'METHODS(.*?)RESULTS'\n",
    "regex_result = r'RESULTS(.*?)CONCLUSIONS'\n",
    "regex_conclusion = r'(?<=CONCLUSIONS)(.*)'\n",
    "regex_dict = {'background': regex_background, 'method': regex_method, \n",
    "              'result': regex_result, 'conclusion': regex_conclusion}\n",
    "\n",
    "splitter = SentenceSplitter(language='en')\n",
    "puncs = '\"#$%&\\'()*+,-/:;<=>@[\\\\]^_`{|}~'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153f74f3-9a0c-4440-8cb2-ed56fb5b367a",
   "metadata": {},
   "source": [
    "#### Define Rouge scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea508028-4b21-4c4b-8e34-8807e22a9cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeLsum'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d73d701-f86b-4dff-a011-8d09b267de26",
   "metadata": {},
   "source": [
    "#### Load CAS-human data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99fc44c-5c56-4aa0-b828-7497928464c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_data = []\n",
    "with open('data/CAS-human/CAS-auto-clean.json', 'r') as f:\n",
    "    for line in tqdm(f):\n",
    "        auto_data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e36612-f7f0-41f0-aea9-12c9e7656871",
   "metadata": {},
   "source": [
    "#### Get splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f825b9e-241d-48ca-99b5-5923b9cc56da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_splits_auto(paper_ids, auto_data):\n",
    "    \n",
    "    all_splits_batch = dict()\n",
    "    init_splits_batch = dict()\n",
    "    \n",
    "    for paper_id in paper_ids:\n",
    "    \n",
    "        processed_abstract = auto_data[paper_id]['abstract']\n",
    "\n",
    "        num_sent = len(processed_abstract)\n",
    "\n",
    "        all_sep_points = list(itertools.combinations(range(num_sent + 1), 2))\n",
    "        all_sep_points = [list(sep_points) for sep_points in all_sep_points]\n",
    "        all_splits = dict()\n",
    "        \n",
    "        for sep_points in all_sep_points:\n",
    "            \n",
    "            if sep_points[1] == num_sent and sep_points[0] in [num_sent-3, num_sent-2, num_sent-1]:\n",
    "                conclusions = processed_abstract[sep_points[0]:]\n",
    "                premises = [sent for sent in processed_abstract if sent not in conclusions]\n",
    "                if {'premise': premises, 'conclusion': conclusions} not in all_splits.values():\n",
    "                    if all(sep <= len(premises) + len(conclusions) for sep in sep_points):\n",
    "                        all_splits[str(sep_points)] = {'premise': premises, 'conclusion': conclusions}\n",
    "                    \n",
    "            elif sep_points[0] == 1 and sep_points[1] in [num_sent-2, num_sent-1]:\n",
    "                premises = processed_abstract[sep_points[0]:sep_points[1]]\n",
    "                conclusions = [sent for sent in processed_abstract if sent not in premises]\n",
    "                if {'premise': premises, 'conclusion': conclusions} not in all_splits.values():\n",
    "                    if all(sep <= len(premises) + len(conclusions) for sep in sep_points):\n",
    "                        all_splits[str(sep_points)] = {'premise': premises, 'conclusion': conclusions}\n",
    "           \n",
    "            elif sep_points[0] == 2 and sep_points[1] == num_sent-1:\n",
    "                premises = processed_abstract[sep_points[0]:sep_points[1]]\n",
    "                conclusions = [sent for sent in processed_abstract if sent not in premises]\n",
    "                if {'premise': premises, 'conclusion': conclusions} not in all_splits.values():\n",
    "                    if all(sep <= len(premises) + len(conclusions) for sep in sep_points):\n",
    "                        all_splits[str(sep_points)] = {'premise': premises, 'conclusion': conclusions}\n",
    "                    \n",
    "                \n",
    "        if all_splits:\n",
    "        \n",
    "            selected_sep_points = list(all_splits.keys())\n",
    "            random_init_sep_points = selected_sep_points[np.random.choice(len(selected_sep_points), 1)[0]]\n",
    "\n",
    "            init_splits_batch[paper_id] = {'sep_points': random_init_sep_points, \n",
    "                                           'premise': all_splits[random_init_sep_points]['premise'], \n",
    "                                           'conclusion': all_splits[random_init_sep_points]['conclusion']}\n",
    "\n",
    "            all_splits_batch[paper_id] = all_splits\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return all_splits_batch, init_splits_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1441d91e-4f67-43f6-9096-45aa8eb957c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_prob(word, tokenized_corpus):\n",
    "    num_occur = tokenized_corpus.count(word)\n",
    "    \n",
    "    prob = num_occur / len(tokenized_corpus)\n",
    "    \n",
    "    if prob > 1e-10:\n",
    "        return prob\n",
    "    \n",
    "    else:\n",
    "        return 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bd6f62-105e-43a1-9da5-480e1fbf9aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self ):\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {}\n",
    "        self.vocab_size = 0\n",
    "    \n",
    "    def sent2seq( self, sent ):\n",
    "        seq = [] # list of word indices\n",
    "        words = sent.split()\n",
    "        for w in words:\n",
    "            if w not in self.word_to_index and w != '.':\n",
    "                self.word_to_index[w] = self.vocab_size\n",
    "                self.index_to_word[self.vocab_size] = w\n",
    "                self.vocab_size +=1\n",
    "            if w != '.':\n",
    "                seq.append( self.word_to_index[w] )\n",
    "        return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9f730b-51f8-460f-b647-3301904b862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_splits(splits, vocab):\n",
    "    indexed_splits = dict()\n",
    "    \n",
    "    for paper_id, split in splits.items():\n",
    "        indexed_splits[paper_id] = {\"premise\": [ vocab.sent2seq(sent) for sent in split['premise'] ], \n",
    "                                    \"conclusion\": [ vocab.sent2seq(sent) for sent in split['conclusion']]\n",
    "                                   }\n",
    "    \n",
    "    return indexed_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14269788-8a94-493c-b6b7-342f3dca540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_list(indexed_splits):\n",
    "    premise_doc_list = []\n",
    "    conclusion_doc_list = []\n",
    "    \n",
    "    for paper_id in indexed_splits.keys():\n",
    "        premise_doc_list.append(indexed_splits[paper_id]['premise'])\n",
    "        conclusion_doc_list.append(indexed_splits[paper_id]['conclusion']) \n",
    "        \n",
    "    return premise_doc_list, conclusion_doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc5bdfb-b66f-49dd-ab55-f595176acc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mi(pre_doc_list, con_doc_list, vocab_size, alpha=\"-inf\", normalized=False): \n",
    "    \n",
    "    num_docs = len(pre_doc_list)\n",
    "    inv_idx_premise  = np.zeros( (num_docs, vocab_size), dtype = np.int32 )\n",
    "    inv_idx_conclusion  = np.zeros( (num_docs, vocab_size), dtype = np.int32 )\n",
    "    \n",
    "    for doc_id, doc in enumerate(pre_doc_list):\n",
    "        all_seqs = []\n",
    "        for seq in doc:\n",
    "            all_seqs += seq\n",
    "        tok_ids, counts = np.unique( all_seqs, return_counts=True )\n",
    "        if len(tok_ids)>0:\n",
    "            inv_idx_premise[doc_id][ tok_ids ] = counts\n",
    "            \n",
    "    for doc_id, doc in enumerate(con_doc_list):\n",
    "        all_seqs = []\n",
    "        for seq in doc:\n",
    "            all_seqs += seq\n",
    "        tok_ids, counts = np.unique( all_seqs, return_counts=True )\n",
    "        if len(tok_ids)>0:\n",
    "            inv_idx_conclusion[doc_id][ tok_ids ] = counts\n",
    "                \n",
    "    wp_wc_count_prod_per_doc = inv_idx_premise[:,:, np.newaxis] * inv_idx_conclusion[:,np.newaxis,:]\n",
    "    wp_wc_count_prod_sum_over_docs = wp_wc_count_prod_per_doc.sum(axis = 0)\n",
    "        \n",
    "    num_words_per_premise = inv_idx_premise.sum(axis = 1)\n",
    "    num_words_per_conclusion = inv_idx_conclusion.sum(axis = 1)\n",
    "    \n",
    "    num_words_prod_per_doc = num_words_per_premise * num_words_per_conclusion\n",
    "    \n",
    "    wp_count_sum_over_docs = inv_idx_premise.sum(axis = 0)\n",
    "    wc_count_sum_over_docs = inv_idx_conclusion.sum(axis = 0)\n",
    "    \n",
    "    assert wp_count_sum_over_docs.sum() == num_words_per_premise.sum()\n",
    "    assert wc_count_sum_over_docs.sum() == num_words_per_conclusion.sum()\n",
    "    assert wp_wc_count_prod_sum_over_docs.sum() == num_words_prod_per_doc.sum()\n",
    "\n",
    "    unique_counts = np.unique(wp_wc_count_prod_sum_over_docs)\n",
    "    \n",
    "    count_histo = dict()\n",
    "    for unique in unique_counts:\n",
    "        unique_count = len(np.where(wp_wc_count_prod_sum_over_docs[wp_wc_count_prod_sum_over_docs == unique])[0])\n",
    "        count_histo[unique] = unique_count\n",
    "            \n",
    "    P_wp_wc = wp_wc_count_prod_sum_over_docs /  num_words_prod_per_doc.sum()\n",
    "    P_wp = wp_count_sum_over_docs / num_words_per_premise.sum()\n",
    "    P_wc = wc_count_sum_over_docs / num_words_per_conclusion.sum()\n",
    "    P_wp_x_P_wc = P_wp[:,np.newaxis] * P_wc[ np.newaxis, : ]\n",
    "   \n",
    "    if normalized == False:\n",
    "        \n",
    "        mi = P_wp_wc * ( np.log( P_wp_wc + 1e-9  ) - np.log( P_wp_x_P_wc + 1e-9  ) )\n",
    "        mi = mi.sum()\n",
    "\n",
    "        return mi\n",
    "    \n",
    "    else:\n",
    "        mi = P_wp_wc * ( np.log( P_wp_wc + 1e-9  ) - np.log( P_wp_x_P_wc + 1e-9  ) )        \n",
    "                \n",
    "        mi = mi.sum()\n",
    "\n",
    "        U_wp = - np.sum( P_wp * np.log(P_wp + 1e-9) )\n",
    "        U_wc = - np.sum( P_wc * np.log(P_wc + 1e-9) )\n",
    "\n",
    "\n",
    "        if alpha == \"-inf\":\n",
    "            U_alpha = np.min([U_wc, U_wp])\n",
    "\n",
    "        elif alpha == -1:\n",
    "            U_alpha =  (2 * U_wc * U_wp) / (U_wc + U_wp )\n",
    "\n",
    "        elif alpha == 0:\n",
    "            U_alpha = np.sqrt(U_wc * U_wp)\n",
    "\n",
    "        elif alpha == 1:\n",
    "            U_alpha = (U_wp + U_wc) / 2\n",
    "\n",
    "        elif alpha == 2:\n",
    "            U_alpha = np.sqrt( ((U_wc * U_wc + U_wp * U_wp) / 2) )\n",
    "\n",
    "        elif alpha == \"inf\":\n",
    "            U_alpha = np.max([U_wc, U_wp])\n",
    "\n",
    "        normalized_mi = mi / (U_alpha + 1e-9)\n",
    "\n",
    "        return normalized_mi, count_histo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad9ec5-5a31-4d15-a206-2474799ffccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_greedy_batches(doc_list, batch_size):\n",
    "    greedy_batches = []\n",
    "    \n",
    "    for i in range(0, len(doc_list), batch_size):\n",
    "        greedy_batches.append(doc_list[i:i+batch_size])\n",
    "        \n",
    "    return greedy_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ff3f22-cfec-4c2e-aab9-f8be4c2e80b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mini_batches(paper_list, batch_size=100):\n",
    "    np.random.shuffle(paper_list)\n",
    "    mini_batches = []\n",
    "    \n",
    "    for i in range(0, len(paper_list), batch_size):\n",
    "        mini_batches.append(paper_list[i:i+batch_size])\n",
    "        \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca2c1bb-f7b0-4b37-8056-dc5cf122bea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mi_single_split(doc_id, sep_point, split, curr_splits, vocab, alpha, normalized):\n",
    "    temp_splits = curr_splits.copy()\n",
    "    temp_splits[doc_id] = {'sep_points': sep_point, \n",
    "                           'premise': split['premise'], 'conclusion': split['conclusion']}\n",
    "    indexed_splits = index_splits(temp_splits, vocab)\n",
    "    pre_doc_list, con_doc_list = get_doc_list(indexed_splits)\n",
    "    vocab_size = vocab.vocab_size\n",
    "    mi, count_histo = compute_mi(pre_doc_list, con_doc_list, vocab_size, alpha, normalized)\n",
    "    \n",
    "    return {'mi': mi, 'split': temp_splits, 'histo':count_histo}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876b82a4-b504-4a7e-950c-06b5c3f28603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def circled_greedy_split_parallel(all_splits, curr_splits, alpha, normalized):\n",
    "    \n",
    "    vocab = Vocab()\n",
    "    \n",
    "    \n",
    "    indexed_splits = index_splits(curr_splits, vocab)\n",
    "    pre_doc_list, con_doc_list = get_doc_list(indexed_splits)\n",
    "    vocab_size = vocab.vocab_size\n",
    "     \n",
    "    best_mi = 0\n",
    "    \n",
    "    best_splits = dict()\n",
    "    best_histo = []\n",
    "    paper_ids = list(curr_splits.keys()) \n",
    "    \n",
    "    np.random.permutation(paper_ids)\n",
    "    \n",
    "    for paper in paper_ids:\n",
    "        sep_points = list(all_splits[paper].keys())\n",
    "        splits = list(all_splits[paper].values())\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            results = executor.map(compute_mi_single_split, \n",
    "                                   [paper]*len(splits), \n",
    "                                   sep_points, \n",
    "                                   splits, \n",
    "                                   [curr_splits]*len(splits), \n",
    "                                   [vocab]*len(splits), \n",
    "                                   [alpha]*len(splits), \n",
    "                                   [normalized]*len(splits))\n",
    "            \n",
    "            results = list(results)\n",
    "            local_mis = [result['mi'] for result in results if result]\n",
    "            local_splits = [result['split'] for result in results if result]\n",
    "            \n",
    "            best_local_mi = np.max(local_mis)\n",
    "            best_local_idx = np.argmax(local_mis)\n",
    "            \n",
    "            if best_local_mi > best_mi:\n",
    "                best_mi = best_local_mi\n",
    "                curr_splits = local_splits[best_local_idx]\n",
    "                best_splits = curr_splits\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            best_histo = [result['histo'] for result in results if result]\n",
    "            \n",
    "    return best_splits, best_mi, best_histo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58fd7d5-431b-499a-8769-9ddf87badb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_seeds(seed_range, num_seeds):\n",
    "    random_seeds = []\n",
    "    for i in range(num_seeds):\n",
    "        random_seeds.append(np.random.choice(range(seed_range), 1, replace=False)[0])\n",
    "\n",
    "    return random_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efdd7af-6337-4e0a-96c8-a88fbaf4a7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cartesian():\n",
    "    def __init__(self, data_group):\n",
    "        self.data_group = data_group\n",
    "        self.counter_idx = len(data_group) - 1\n",
    "        self.counter = [0 for i in range(0, len(self.data_group))]\n",
    "        \n",
    "    def count_length(self):\n",
    "        i = 0\n",
    "        length = 1\n",
    "        while i < len(self.data_group):\n",
    "            length *= len(self.data_group[i])\n",
    "            i += 1\n",
    "        return length\n",
    "    \n",
    "    def handle(self):\n",
    "        self.counter[self.counter_idx] += 1\n",
    "        if self.counter[self.counter_idx] >= len(self.data_group[self.counter_idx]):\n",
    "            self.counter[self.counter_idx] = 0\n",
    "            self.counter_idx -= 1\n",
    "            if self.counter_idx >= 0:\n",
    "                self.handle()\n",
    "            self.counter_idx = len(self.data_group) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fe239c-6371-4def-b594-87c6ed10d0d1",
   "metadata": {},
   "source": [
    "#### SentenceBERT to find the most relevant abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bed5582-d7e9-4b8a-9147-61f2c9a668b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTransformersNNSearch:\n",
    "    def __init__( self, model_name=\"all-MiniLM-L6-v2\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        \n",
    "    def encode( self, sentences):\n",
    "        return self.model.encode(sentences)\n",
    "    \n",
    "    def normalize_embeddings(self, embeddings ):\n",
    "        assert len( embeddings.shape ) == 2\n",
    "        normalized_embeddings = embeddings /(np.linalg.norm( embeddings, axis =1, keepdims=True )+1e-12)\n",
    "        return normalized_embeddings\n",
    "    \n",
    "    def rank_sentences( self, source_sentence, target_sentences, top_n=None, reverse=False  ):\n",
    "        source_embedding = self.normalize_embeddings(self.encode([source_sentence]))[0]\n",
    "        target_embeddings = self.normalize_embeddings(self.encode(target_sentences))\n",
    "        sim_scores = np.dot(target_embeddings, source_embedding)\n",
    "        if top_n is None:\n",
    "            top_n = len(target_sentences)\n",
    "        if reverse is False:\n",
    "            I = np.argsort(-sim_scores)[:top_n]\n",
    "            D = sim_scores[I]\n",
    "        elif reverse is True:\n",
    "            I = np.argsort(sim_scores)[:top_n]\n",
    "            D = sim_scores[I]\n",
    "        return D, I "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dede858-ad4f-4e48-866a-3e574664a5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SentBert_model = SentenceTransformersNNSearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77724ed-176a-4bab-9588-4da0bb1a8a69",
   "metadata": {},
   "source": [
    "#### Exhaustive abstract slicing on the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f132c814-85e5-459c-9545-630ad860417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c046aefe-5af4-4fa7-85aa-1044a32c2c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_distance(A, B):\n",
    "    #Find intersection of two sets\n",
    "    A = set(A)\n",
    "    B = set(B)\n",
    "    \n",
    "    nominator = A.intersection(B)\n",
    "    denominator = A.union(B)\n",
    "    similarity = len(nominator)/len(denominator)\n",
    "    \n",
    "    return 1 - similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f014ba47-09c2-45fe-90ca-70cb802e8a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segment_boundary(input_seq):\n",
    "    ### input: seq of labels, 1 for conclusion and 0 for premise, e.g. [1, 0, 0, 0, 1] indicates that the first and the last sentence in the abstract are conclusions\n",
    "    ### output: seq of labels, 1 for boundary and 0 for non-boundary. e.g. [1, 0, 0, 0, 1] ==> [1, 0, 0, 1, 0]\n",
    "    \n",
    "    output_seq = np.zeros(len(input_seq))\n",
    "    \n",
    "    for i in range(len(input_seq)-1):\n",
    "        if input_seq[i+1] != input_seq[i]:\n",
    "            output_seq[i] = 1\n",
    "        else:\n",
    "            output_seq[i] = 0\n",
    "    if input_seq[-1] == input_seq[0]:\n",
    "        output_seq[-1] = 0\n",
    "    else:\n",
    "        output_seq[-1] = 1\n",
    "    \n",
    "    return output_seq "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2293b00c-5ed3-46a8-bd0f-17b526a05426",
   "metadata": {},
   "source": [
    "#### GreedyCAS-plus (with NN search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a0a939-a2fe-4007-aaab-34dbd6dee812",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_list = range(0, len(auto_data))\n",
    "\n",
    "chunk_size = 100\n",
    "batch_size = 12\n",
    "num_epochs = 5\n",
    "seed_size = int(chunk_size / batch_size)\n",
    "\n",
    "all_chunks = list()\n",
    "for i in range(0, len(paper_list), chunk_size):\n",
    "    all_chunks.append(paper_list[i:i+chunk_size])\n",
    "    \n",
    "print(\"chunking finished! \\n\")\n",
    "\n",
    "jaccard_dist_all = []\n",
    "pk_score_all = []\n",
    "rouge_score_all = []\n",
    "windiff_score_all = []\n",
    "mi_score_all = []\n",
    "        \n",
    "while len(paper_list) != 0:\n",
    "    \n",
    "    chunk_counter = 0\n",
    "    for current_chunk in all_chunks:\n",
    "        print('===========================================================================')\n",
    "        print(f'remaining abstracts to slice:{len(paper_list)}')\n",
    "        print(f\"current chunk processing: {chunk_counter}\")\n",
    "\n",
    "        seed_papers = np.random.choice(current_chunk, seed_size, replace=False)\n",
    "        current_chunk_abstracts = dict()\n",
    "        for paper in current_chunk:\n",
    "            current_chunk_abstracts[paper] = \" \".join(auto_data[paper]['conclusion'])\n",
    "        \n",
    "        chunk_paper_ids = list()\n",
    "        for paper in seed_papers:\n",
    "            source_abstract = \" \".join(auto_data[paper]['abstract'])\n",
    "            target_abstracts = list(current_chunk_abstracts.values())\n",
    "            D, I = SentBert_model.rank_sentences( source_abstract, target_abstracts, top_n=batch_size, reverse=False )\n",
    "            chunk_idx = np.array(list(current_chunk_abstracts.keys()))[I]\n",
    "            chunk_idx.tolist().insert(0, paper)\n",
    "            chunk_paper_ids.append(list(chunk_idx))\n",
    "            \n",
    "        paper_list = [rest_paper for rest_paper in paper_list if rest_paper not in current_chunk]\n",
    "                    \n",
    "        chunk_pk_scores = []\n",
    "        chunk_windiff_scores = []\n",
    "        chunk_jaccard_scores = []\n",
    "        chunk_rouge_scores = []\n",
    "        \n",
    "        for batch_paper_ids in chunk_paper_ids:\n",
    "            \n",
    "            ground_truth = dict()\n",
    "            \n",
    "            ref_bounds_batch = dict()\n",
    "            jaccard_ref_batch = []\n",
    "            \n",
    "            for paper_id in batch_paper_ids:\n",
    "                \n",
    "                try:\n",
    "                    \n",
    "                    true_conclusion = auto_data[paper_id]['conclusion']\n",
    "                    \n",
    "                    jaccard_ref_batch.append(true_conclusion)\n",
    "                    \n",
    "                    true_premise = [sent for sent in auto_data[paper_id]['abstract'] if sent not in auto_data[paper_id]['conclusion']]\n",
    "\n",
    "                    true_bound = np.zeros(len(auto_data[paper_id]['abstract']))\n",
    "                    \n",
    "                    true_bound[-len(true_conclusion):] = 1\n",
    "                    ref_bounds_batch[paper_id] = get_segment_boundary(true_bound)\n",
    "                    \n",
    "                except KeyError:\n",
    "                    continue\n",
    "                                \n",
    "            # randomly start the batch configuration\n",
    "            all_splits, curr_splits = get_all_splits_auto(batch_paper_ids, auto_data)\n",
    "            \n",
    "            best_mi = 0\n",
    "            best_splits = dict()\n",
    "            \n",
    "            for epoch in range(num_epochs):\n",
    "                splits, mi, histo = circled_greedy_split_parallel(all_splits, curr_splits, alpha=\"-inf\", normalized=True)\n",
    "                \n",
    "                if mi > best_mi:\n",
    "                    best_mi = mi\n",
    "                    mi_score_all.append(best_mi)\n",
    "                    best_splits = splits\n",
    "                    curr_splits = splits\n",
    "                    \n",
    "                else:\n",
    "                    all_splits, curr_splits = get_all_splits_auto(batch_paper_ids, auto_data)\n",
    "                    \n",
    "            jaccard_scores = list()\n",
    "            \n",
    "            ### construct boundaries for evaluating with pk\n",
    "            ### hyp_bound: seq for the best configuration, e.g. [0, 0, 0, 1, 0, 0, 1] 0 => not a boundary sentence, 1 => a boundary sentence\n",
    "            ### true_bound: seq for the ground truth config, e.g. [0, 0, 0, 0, 0, 1, 1] 0 => not a boundary sentence, 1 => a boundary sentence\n",
    "            \n",
    "            hyp_bounds_batch = dict()\n",
    "            jaccard_hyp_batch = []\n",
    "            rouge_score_batch = []\n",
    "            \n",
    "            for key in batch_paper_ids:\n",
    "                hyp_bound = np.zeros(len(auto_data[key]['abstract']))\n",
    "                hyp_conclusion = best_splits[key]['conclusion']\n",
    "                jaccard_hyp_batch.append(hyp_conclusion)\n",
    "                try:\n",
    "                    hyp_bound[ast.literal_eval(best_splits[key]['sep_points'])[0]-1] = 1\n",
    "                    hyp_bound[ast.literal_eval(best_splits[key]['sep_points'])[1]-1] = 1\n",
    "                    hyp_bounds_batch[key] = hyp_bound\n",
    "        \n",
    "                except IndexError:\n",
    "                    print(best_splits)\n",
    "                \n",
    "            batch_pk_scores = []\n",
    "            for key in best_splits:\n",
    "                batch_pk_scores.append(pk(\"\".join(str(int(item)) for item in hyp_bounds_batch[key]), \"\".join(str(int(item)) for item in ref_bounds_batch[key]), 2))\n",
    "            \n",
    "            avg_batch_pk = np.mean(batch_pk_scores)\n",
    "            \n",
    "            batch_windiff_scores = []\n",
    "            for key in best_splits:\n",
    "                batch_windiff_scores.append(windowdiff(\"\".join(str(int(item)) for item in hyp_bounds_batch[key]), \"\".join(str(int(item)) for item in ref_bounds_batch[key]), 2))\n",
    "            \n",
    "            avg_batch_windiff = np.mean(batch_windiff_scores)\n",
    "            \n",
    "            batch_jaccard_scores = []\n",
    "            batch_rouge_scores = []\n",
    "            for hyp, ref in zip(jaccard_hyp_batch, jaccard_ref_batch):\n",
    "                \n",
    "                batch_jaccard_scores.append(jaccard_distance(ref, hyp))\n",
    "                rouge_score = scorer.score(\" \".join(ref), \" \".join(hyp))\n",
    "                \n",
    "                rouge1_fm = rouge_score['rouge1'][2]\n",
    "                rouge2_fm = rouge_score['rouge2'][2]\n",
    "                rougeL_fm = rouge_score['rougeLsum'][2]\n",
    "                batch_rouge_scores.append((rouge1_fm + rouge2_fm + rougeL_fm)/3)\n",
    "                \n",
    "            avg_batch_jaccard = np.mean(batch_jaccard_scores)\n",
    "            \n",
    "            avg_batch_rouge = np.mean(batch_rouge_scores)\n",
    "            \n",
    "            print(f\"averaged Pk on the {chunk_paper_ids.index(batch_paper_ids)}.th batch: {avg_batch_pk}\")\n",
    "            chunk_pk_scores.append(batch_pk_scores)\n",
    "            print(f\"averaged wd on the {chunk_paper_ids.index(batch_paper_ids)}.th batch: {avg_batch_windiff}\")\n",
    "            chunk_windiff_scores.append(batch_windiff_scores)\n",
    "            print(f\"averaged Jaccard on the {chunk_paper_ids.index(batch_paper_ids)}.th batch: {avg_batch_jaccard}\")\n",
    "            chunk_jaccard_scores.append(batch_jaccard_scores)\n",
    "            print(f\"averaged Rouge on the {chunk_paper_ids.index(batch_paper_ids)}.th batch: {avg_batch_rouge}\")\n",
    "            chunk_rouge_scores.append(batch_rouge_scores)\n",
    "            \n",
    "        print(f\"averaged Pk of the {chunk_counter}.th chunk: {np.mean(chunk_pk_scores)}\")\n",
    "        print(f\"averaged wd of the {chunk_counter}.th chunk: {np.mean(chunk_windiff_scores)}\")\n",
    "        print(f\"averaged Jaccard of the {chunk_counter}.th chunk: {np.mean(chunk_jaccard_scores)}\")\n",
    "        print(f\"averaged Rouge of the {chunk_counter}.th chunk: {np.mean(chunk_rouge_scores)}\")\n",
    "        pk_score_all.append(chunk_pk_scores)\n",
    "        windiff_score_all.append(chunk_windiff_scores)\n",
    "        jaccard_dist_all.append(chunk_jaccard_scores)\n",
    "        rouge_score_all.append(chunk_rouge_scores)\n",
    "        \n",
    "        chunk_counter += 1\n",
    "        \n",
    "print(f\"overall Pk score for GreedyCAS-plus: {np.mean(pk_score_all)}\")\n",
    "print(f\"overall wd scores for GreedyCAS-plus: {np.mean(windiff_score_all)}\")\n",
    "print(f\"overall Jaccard index for GreedyCAS-plus: {1 - np.mean(jaccard_dist_all)}\")\n",
    "print(f\"overall Rouge scores for GreedyCAS-plus: {np.mean(rouge_score_all)}\")\n",
    "print(f\"overall NMI scores for GreedyCAS-plus: {np.mean(mi_score_all)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2034b019-a6f7-4e0a-8a6a-51fa884e2d8f",
   "metadata": {},
   "source": [
    "#### GreedyCAS-base (no NN search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5f44f3-e954-4980-8e03-b7dbdc719ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_list = range(0, len(auto_data))\n",
    "\n",
    "chunk_size = 100\n",
    "batch_size = 11\n",
    "num_epochs = 5\n",
    "seed_size = int(chunk_size / batch_size)\n",
    "\n",
    "all_chunks = list()c\n",
    "for i in range(0, len(paper_list), chunk_size):\n",
    "    all_chunks.append(paper_list[i:i+chunk_size])\n",
    "    \n",
    "print(\"chunking finished! \\n\")\n",
    "\n",
    "jaccard_dist_all = []\n",
    "pk_score_all = []\n",
    "rouge_score_all = []\n",
    "windiff_score_all = []\n",
    "mi_score_all = []\n",
    "count_histo_all = []\n",
    "        \n",
    "while len(paper_list) != 0:\n",
    "    \n",
    "    chunk_counter = 0\n",
    "    for current_chunk in all_chunks:\n",
    "        print('===========================================================================')\n",
    "        print(f'remaining abstracts to slice:{len(paper_list)}')\n",
    "        print(f\"current chunk processing: {chunk_counter}\")\n",
    "\n",
    "        chunk_paper_ids = list()\n",
    "#         \n",
    "        for i in range(0, len(current_chunk), batch_size):\n",
    "            chunk_paper_ids.append(list(current_chunk[i:i+batch_size]))\n",
    "            \n",
    "        paper_list = [rest_paper for rest_paper in paper_list if rest_paper not in current_chunk]\n",
    "        \n",
    "        chunk_pk_scores = []\n",
    "        chunk_windiff_scores = []\n",
    "        chunk_jaccard_scores = []\n",
    "        chunk_rouge_scores = []\n",
    "        chunk_mi_scores = []\n",
    "        \n",
    "        for batch_paper_ids in chunk_paper_ids:\n",
    "            \n",
    "            ground_truth = dict()\n",
    "            \n",
    "            ref_bounds_batch = dict()\n",
    "            jaccard_ref_batch = []\n",
    "            \n",
    "            for paper_id in batch_paper_ids:\n",
    "                \n",
    "                try:\n",
    "\n",
    "                    true_conclusion = auto_data[paper_id]['conclusion']\n",
    "                                        \n",
    "                    jaccard_ref_batch.append(true_conclusion)\n",
    "                    true_premise = [sent for sent in auto_data[paper_id]['abstract'] if sent not in auto_data[paper_id]['conclusion']]\n",
    "\n",
    "                    true_bound = np.zeros(len(auto_data[paper_id]['abstract']))\n",
    "                    \n",
    "                    true_bound[-len(true_conclusion):] = 1\n",
    "                    ref_bounds_batch[paper_id] = get_segment_boundary(true_bound)\n",
    "                    \n",
    "                except KeyError:\n",
    "                    continue\n",
    "                                \n",
    "            # randomly start the batch configuration\n",
    "            all_splits, curr_splits = get_all_splits_auto(batch_paper_ids, auto_data)\n",
    "            \n",
    "            best_mi = 0\n",
    "            best_splits = dict()\n",
    "            best_histo = dict()\n",
    "            \n",
    "            for epoch in range(num_epochs):\n",
    "                splits, mi, histo = circled_greedy_split_parallel(all_splits, curr_splits, alpha=\"-inf\", normalized=True)\n",
    "                \n",
    "                if mi > best_mi:\n",
    "                    best_mi = mi\n",
    "                    best_histo = histo\n",
    "                    mi_score_all.append(best_mi)\n",
    "                    best_splits = splits\n",
    "                    curr_splits = splits\n",
    "                    \n",
    "                else:\n",
    "                    all_splits, curr_splits = get_all_splits_auto(batch_paper_ids, auto_data)\n",
    "                    \n",
    "            \n",
    "            ### construct boundaries for evaluating with pk\n",
    "            ### hyp_bound: seq for the best configuration, e.g. [0, 0, 0, 1, 0, 0, 1] 0 => not a boundary sentence, 1 => a boundary sentence\n",
    "            ### true_bound: seq for the ground truth config, e.g. [0, 0, 0, 0, 0, 1, 1] 0 => not a boundary sentence, 1 => a boundary sentence\n",
    "            \n",
    "            hyp_bounds_batch = dict()\n",
    "            jaccard_hyp_batch = []\n",
    "            rouge_score_batch = []\n",
    "            mi_batch = best_mi\n",
    "            \n",
    "            for key in best_splits:\n",
    "                hyp_bound = np.zeros(len(auto_data[key]['abstract']))\n",
    "                \n",
    "                hyp_conclusion = best_splits[key]['conclusion']\n",
    "                \n",
    "                jaccard_hyp_batch.append(hyp_conclusion)\n",
    "                \n",
    "                try:\n",
    "                    hyp_bound[ast.literal_eval(best_splits[key]['sep_points'])[0]-1] = 1\n",
    "                    hyp_bound[ast.literal_eval(best_splits[key]['sep_points'])[1]-1] = 1\n",
    "                    hyp_bounds_batch[key] = hyp_bound\n",
    "                except IndexError:\n",
    "                    print(best_splits[key])\n",
    "                \n",
    "            batch_pk_scores = []\n",
    "            for key in best_splits:\n",
    "                batch_pk_scores.append(pk(\"\".join(str(int(item)) for item in hyp_bounds_batch[key]), \"\".join(str(int(item)) for item in ref_bounds_batch[key]), 2))\n",
    "            \n",
    "            avg_batch_pk = np.mean(batch_pk_scores)\n",
    "            \n",
    "            batch_windiff_scores = []\n",
    "            for key in best_splits:\n",
    "                batch_windiff_scores.append(windowdiff(\"\".join(str(int(item)) for item in hyp_bounds_batch[key]), \"\".join(str(int(item)) for item in ref_bounds_batch[key]), 2))\n",
    "            \n",
    "            avg_batch_windiff = np.mean(batch_windiff_scores)\n",
    "            \n",
    "            batch_jaccard_scores = []\n",
    "            batch_rouge_scores = []\n",
    "            \n",
    "            for hyp, ref in zip(jaccard_hyp_batch, jaccard_ref_batch):\n",
    "                \n",
    "                batch_jaccard_scores.append(jaccard_distance(ref, hyp))\n",
    "                rouge_score = scorer.score(\" \".join(ref), \" \".join(hyp))\n",
    "                rouge1_fm = rouge_score['rouge1'][2]\n",
    "                rouge2_fm = rouge_score['rouge2'][2]\n",
    "                rougeL_fm = rouge_score['rougeLsum'][2]\n",
    "                batch_rouge_scores.append((rouge1_fm + rouge2_fm + rougeL_fm)/3)\n",
    "                \n",
    "            avg_batch_jaccard = np.mean(batch_jaccard_scores)\n",
    "            \n",
    "            avg_batch_rouge = np.mean(batch_rouge_scores)\n",
    "            \n",
    "            print(f\"averaged Pk on the {chunk_paper_ids.index(batch_paper_ids)}.th batch: {avg_batch_pk}\")\n",
    "            chunk_pk_scores.append(avg_batch_pk)\n",
    "            print(f\"averaged wd on the {chunk_paper_ids.index(batch_paper_ids)}.th batch: {avg_batch_windiff}\")\n",
    "            chunk_windiff_scores.append(avg_batch_windiff)\n",
    "            print(f\"averaged Jaccard on the {chunk_paper_ids.index(batch_paper_ids)}.th batch: {avg_batch_jaccard}\")\n",
    "            chunk_jaccard_scores.append(avg_batch_jaccard)\n",
    "            print(f\"averaged Rouge on the {chunk_paper_ids.index(batch_paper_ids)}.th batch: {avg_batch_rouge}\")\n",
    "            chunk_rouge_scores.append(avg_batch_rouge)\n",
    "            print(f\"averaged MI on the {chunk_paper_ids.index(batch_paper_ids)}.th batch: {mi_batch}\")\n",
    "            chunk_mi_scores.append(mi_batch)\n",
    "            \n",
    "            count_histo_all.extend(best_histo)\n",
    "            \n",
    "        print(f\"averaged Pk of the {chunk_counter}.th chunk: {np.mean(chunk_pk_scores)}\")\n",
    "        print(f\"averaged wd of the {chunk_counter}.th chunk: {np.mean(chunk_windiff_scores)}\")\n",
    "        print(f\"averaged Jaccard of the {chunk_counter}.th chunk: {np.mean(chunk_jaccard_scores)}\")\n",
    "        print(f\"averaged Rouge of the {chunk_counter}.th chunk: {np.mean(chunk_rouge_scores)}\")\n",
    "        print(f\"averaged MI of the {chunk_counter}.th chunk: {np.mean(chunk_mi_scores)}\")\n",
    "        \n",
    "        pk_score_all.append(np.mean(chunk_pk_scores))\n",
    "        windiff_score_all.append(np.mean(chunk_windiff_scores))\n",
    "        jaccard_dist_all.append(np.mean(chunk_jaccard_scores))\n",
    "        rouge_score_all.append(np.mean(chunk_rouge_scores))\n",
    "        mi_score_all.append(np.mean(chunk_mi_scores))\n",
    "                \n",
    "        chunk_counter += 1\n",
    "        \n",
    "print(f\"overall pk scores for GreedyCAS-base: {np.mean(pk_score_all)}\")\n",
    "print(f\"overall wd scores for GreedyCAS-base: {np.mean(windiff_score_all)}\")\n",
    "print(f\"overall Jaccard index for GreedyCAS-base: {1 - np.mean(jaccard_dist_all)}\")\n",
    "print(f\"overall Rouge scores for GreedyCAS-base: {np.mean(rouge_score_all)}\")\n",
    "print(f\"overall NMI scores for GreedyCAS-base: {np.mean(mi_score_all)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a8a4a1-dfac-4b45-8eaf-0f73b78e6bcb",
   "metadata": {},
   "source": [
    "#### Random-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11f1601-484b-4a74-a03e-9ebe7fceb774",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_list = range(0, len(auto_data))\n",
    "\n",
    "all_splits, _ = get_all_splits_auto(paper_list, auto_data)\n",
    "\n",
    "ref_bounds_all = dict()       \n",
    "hyp_bounds_all = dict()\n",
    "\n",
    "jaccard_dist_all = []\n",
    "rouge_score_all = []\n",
    "pk_score_all = []\n",
    "windiff_score_all = []\n",
    "\n",
    "\n",
    "for paper_id in tqdm(paper_list):\n",
    "    try:\n",
    "        true_conclusion = auto_data[paper_id]['conclusion']\n",
    "        true_premise = [sent for sent in auto_data[paper_id]['abstract'] if sent not in auto_data[paper_id]['conclusion']]\n",
    "        \n",
    "        true_bound = np.zeros(len(auto_data[paper_id]['abstract']))\n",
    "        true_bound[-len(true_conclusion):] = 1\n",
    "        ref_bounds_all[paper_id] = get_segment_boundary(true_bound)\n",
    "        \n",
    "        true_abstract = true_conclusion + true_premise\n",
    "        \n",
    "        np.random.seed(1)\n",
    "        random_split_boundary = np.random.choice([idx for idx in range(1, len(true_abstract)+1, 1)], size=2, replace=False)\n",
    "        random_split_boundary.sort()\n",
    "        random_split_boundary = [idx for idx in list(random_split_boundary)]\n",
    "        \n",
    "        random_split_segment = dict()\n",
    "        temp = true_abstract[random_split_boundary[0]:random_split_boundary[1]]\n",
    "        \n",
    "        if any(sent in true_abstract[-3:] for sent in temp) :\n",
    "            random_split_segment['conclusion'] = temp\n",
    "            random_split_segment['premise'] = [sent for sent in true_abstract if sent not in temp]\n",
    "        else:\n",
    "            random_split_segment['premise'] = temp\n",
    "            random_split_segment['conclusion'] = [sent for sent in true_abstract if sent not in temp]\n",
    "                            \n",
    "        hyp_bound = np.zeros(len(auto_data[paper_id]['abstract']))\n",
    "        \n",
    "        hyp_conclusion = random_split_segment['conclusion']\n",
    "        \n",
    "        jaccard_dist_all.append(jaccard_distance(hyp_conclusion, true_conclusion))\n",
    "        \n",
    "        rouge_score = scorer.score(\" \".join(true_conclusion), \" \".join(hyp_conclusion))\n",
    "        \n",
    "        rouge1_fm = rouge_score['rouge1'][2]\n",
    "        rouge2_fm = rouge_score['rouge2'][2]\n",
    "        rougeL_fm = rouge_score['rougeLsum'][2]\n",
    "        \n",
    "        rouge_score_all.append((rouge1_fm + rouge2_fm + rougeL_fm)/3)\n",
    "        \n",
    "        try:\n",
    "            hyp_bound[random_split_boundary[0]-1] = 1\n",
    "            hyp_bound[random_split_boundary[1]-1] = 1\n",
    "            hyp_bounds_all[paper_id] = hyp_bound\n",
    "        except IndexError:\n",
    "            print(all_splits[paper_id])   \n",
    "\n",
    "    except KeyError:\n",
    "        continue\n",
    "    \n",
    "    \n",
    "for key in ref_bounds_all.keys():\n",
    "    pk_score_all.append(pk(\"\".join(str(int(item)) for item in hyp_bounds_all[key]), \"\".join(str(int(item)) for item in ref_bounds_all[key]), 2))\n",
    "    windiff_score_all.append(windowdiff(\"\".join(str(int(item)) for item in hyp_bounds_all[key]), \"\".join(str(int(item)) for item in ref_bounds_all[key]), 2))\n",
    "    \n",
    "\n",
    "print(f\"avg. pk scores for random fetch: {np.mean(pk_score_all)}\")\n",
    "print(f\"avg. wd scores for random fetch: {np.mean(windiff_score_all)}\")\n",
    "print(f\"avg. Jaccard index for random fetch: {1 - np.mean(jaccard_dist_all)}\")\n",
    "print(f\"avg. Rouge scores for random fetch: {np.mean(rouge_score_all)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c5983c-f420-404f-8fe1-06284ffacc37",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Random-plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b5f3cc-c52e-4690-b7db-008faceaa886",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_list = range(0, len(auto_data))\n",
    "\n",
    "all_splits, _ = get_all_splits_auto(paper_list, auto_data)\n",
    "\n",
    "ref_bounds_all = dict()       \n",
    "hyp_bounds_all = dict()\n",
    "\n",
    "jaccard_dist_all = []\n",
    "rouge_score_all = []\n",
    "pk_score_all = []\n",
    "windiff_score_all = []\n",
    "\n",
    "indexed_splits = dict()\n",
    "\n",
    "for paper_id in tqdm(paper_list):\n",
    "    try:\n",
    "        true_conclusion = auto_data[paper_id]['conclusion']\n",
    "        true_premise = [sent for sent in auto_data[paper_id]['abstract'] if sent not in auto_data[paper_id]['conclusion']]\n",
    "\n",
    "        true_bound = np.zeros(len(auto_data[paper_id]['abstract']))\n",
    "        true_bound[-len(true_conclusion):] = 1\n",
    "        ref_bounds_all[paper_id] = get_segment_boundary(true_bound)\n",
    "        \n",
    "        random.seed(1)\n",
    "        random_split_boundary, random_split_segment = random.choice(list(all_splits[paper_id].items()))\n",
    "    \n",
    "        hyp_bound = np.zeros(len(auto_data[paper_id]['abstract']))\n",
    "        \n",
    "        hyp_conclusion = random_split_segment['conclusion']\n",
    "        jaccard_dist_all.append(jaccard_distance(hyp_conclusion, true_conclusion))\n",
    "        \n",
    "        rouge_score = scorer.score(\" \".join(true_conclusion), \" \".join(hyp_conclusion))\n",
    "        \n",
    "        rouge1_fm = rouge_score['rouge1'][2]\n",
    "        rouge2_fm = rouge_score['rouge2'][2]\n",
    "        rougeL_fm = rouge_score['rougeLsum'][2]\n",
    "        \n",
    "        rouge_score_all.append((rouge1_fm + rouge2_fm + rougeL_fm)/3)\n",
    "        \n",
    "        indexed_splits[paper_id] = {'premise': [sent for sent in random_split_segment['premise']], \n",
    "                                    'conclusion': [sent for sent in random_split_segment['conclusion']]}\n",
    "        \n",
    "        try:\n",
    "            hyp_bound[ast.literal_eval(random_split_boundary)[0]-1] = 1\n",
    "            hyp_bound[ast.literal_eval(random_split_boundary)[1]-1] = 1\n",
    "            hyp_bounds_all[paper_id] = hyp_bound\n",
    "            \n",
    "        except IndexError:\n",
    "            print(all_splits[paper_id])   \n",
    "\n",
    "    except KeyError:\n",
    "        continue\n",
    "    \n",
    "for key in ref_bounds_all.keys():\n",
    "    pk_score_all.append(pk(\"\".join(str(int(item)) for item in hyp_bounds_all[key]), \"\".join(str(int(item)) for item in ref_bounds_all[key]), 2))\n",
    "    windiff_score_all.append(windowdiff(\"\".join(str(int(item)) for item in hyp_bounds_all[key]), \"\".join(str(int(item)) for item in ref_bounds_all[key]), 2))\n",
    "        \n",
    "print(f\"avg. pk scores for random fetch: {np.mean(pk_score_all)}\")\n",
    "print(f\"avg. wd scores for random fetch: {np.mean(windiff_score_all)}\")\n",
    "print(f\"avg. Jaccard index for random fetch: {1 - np.mean(jaccard_dist_all)}\")\n",
    "print(f\"avg. Rouge scores for random fetch: {np.mean(rouge_score_all)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff12ad69-ff5b-4f61-83b0-72803d524b83",
   "metadata": {},
   "source": [
    "#### SBERT-sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175a4980-b94f-4cc1-bbaf-074222751f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SBERT-sim baseline: choose the configuration where the semantic similarity between premises and conclusions is the greatest\n",
    "\n",
    "paper_list = range(0, len(auto_data))\n",
    "\n",
    "all_splits, _ = get_all_splits_auto(paper_list, auto_data)\n",
    "\n",
    "ref_bounds_all = dict()   \n",
    "hyp_bounds_all = dict()\n",
    "\n",
    "jaccard_dist_all = []\n",
    "rouge_score_all = []\n",
    "pk_score_all = []\n",
    "windiff_score_all = []\n",
    "\n",
    "for paper_id in tqdm(paper_list):\n",
    "    try:\n",
    "        true_conclusion = auto_data[paper_id]['conclusion']\n",
    "        \n",
    "        true_premise = [sent for sent in auto_data[paper_id]['abstract'] if sent not in auto_data[paper_id]['conclusion']]\n",
    "\n",
    "        true_bound = np.zeros(len(auto_data[paper_id]['abstract']))\n",
    "        true_bound[-len(true_conclusion):] = 1\n",
    "        ref_bounds_all[paper_id] = get_segment_boundary(true_bound)\n",
    "                \n",
    "        all_sim_scores = []\n",
    "        all_boundaries = []\n",
    "        \n",
    "        for boundary, split in all_splits[paper_id].items():\n",
    "            premises = \" \".join(split['premise'])\n",
    "            conclusions = \" \".join(split['conclusion'])\n",
    "            premise_embedding = SentBert_model.normalize_embeddings(SentBert_model.encode([premises]))[0]\n",
    "            conclusion_embedding = SentBert_model.normalize_embeddings(SentBert_model.encode([conclusions]))[0]\n",
    "            sim_score = np.dot(premise_embedding, conclusion_embedding)\n",
    "            all_sim_scores.append(sim_score)\n",
    "            all_boundaries.append(boundary)\n",
    "        \n",
    "        max_sim_idx = np.argmax(all_sim_scores)\n",
    "        \n",
    "        hyp_bound = np.zeros(len(auto_data[paper_id]['abstract']))\n",
    "        \n",
    "        hyp_conclusion = all_splits[paper_id][all_boundaries[max_sim_idx]]['conclusion']\n",
    "        jaccard_dist_all.append(1 - jaccard_distance(hyp_conclusion, true_conclusion))\n",
    "        \n",
    "        rouge_score = scorer.score(\" \".join(true_conclusion), \" \".join(hyp_conclusion))\n",
    "        \n",
    "        rouge1_fm = rouge_score['rouge1'][2]\n",
    "        rouge2_fm = rouge_score['rouge2'][2]\n",
    "        rougeL_fm = rouge_score['rougeLsum'][2]\n",
    "        \n",
    "        rouge_score_all.append((rouge1_fm + rouge2_fm + rougeL_fm)/3)\n",
    "        \n",
    "        try:\n",
    "            hyp_bound[ast.literal_eval(all_boundaries[max_sim_idx])[0]-1] = 1\n",
    "            hyp_bound[ast.literal_eval(all_boundaries[max_sim_idx])[1]-1] = 1\n",
    "            hyp_bounds_all[paper_id] = hyp_bound\n",
    "            \n",
    "        except IndexError:\n",
    "            print(all_splits[paper_id])\n",
    "        \n",
    "\n",
    "    except KeyError:\n",
    "        continue\n",
    "        \n",
    "all_random_pk = []\n",
    "\n",
    "\n",
    "for key in ref_bounds_all.keys():\n",
    "    pk_score_all.append(pk(\"\".join(str(int(item)) for item in hyp_bounds_all[key]), \"\".join(str(int(item)) for item in ref_bounds_all[key]), 2))\n",
    "    windiff_score_all.append(windowdiff(\"\".join(str(int(item)) for item in hyp_bounds_all[key]), \"\".join(str(int(item)) for item in ref_bounds_all[key]), 2))\n",
    "    \n",
    "print(f\"avg. pk scores for SBERT: {np.mean(pk_score_all)}\")\n",
    "print(f\"avg. wd scores for SBERT: {np.mean(windiff_score_all)}\")\n",
    "print(f\"avg. Jaccard index for SBERT: {np.mean(jaccard_dist_all)}\")\n",
    "print(f\"avg. Rouge scores for SBERT: {np.mean(rouge_score_all)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5206dd03-e641-41dc-a3b1-2888e730c750",
   "metadata": {},
   "source": [
    "#### TextTiling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9f762-77ec-4bcd-92c3-9ce6d909521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TextTiling baseline: split abstract based on lexical cohesion between segments\n",
    "\n",
    "paper_list = range(0, len(auto_data))\n",
    "\n",
    "all_splits, _ = get_all_splits_auto(paper_list, auto_data)\n",
    "\n",
    "ref_bounds_all = dict()         \n",
    "hyp_bounds_all = dict()\n",
    "\n",
    "ht = HarvestText(language=\"en\")\n",
    "\n",
    "jaccard_dist_all = []\n",
    "rouge_score_all = []\n",
    "pk_score_all = []\n",
    "windiff_score_all = []\n",
    "\n",
    "for paper_id in tqdm(paper_list):\n",
    "    try:\n",
    "        true_conclusion = auto_data[paper_id]['conclusion']\n",
    "\n",
    "        true_premise = [sent for sent in auto_data[paper_id]['abstract'] if sent not in auto_data[paper_id]['conclusion']]\n",
    "        \n",
    "        true_abstract = true_premise + true_conclusion\n",
    "        \n",
    "        true_bound = np.zeros(len(auto_data[paper_id]['abstract']))\n",
    "        true_bound[-len(true_conclusion):] = 1\n",
    "        \n",
    "        ref_bounds_all[paper_id] = get_segment_boundary(true_bound)\n",
    "        \n",
    "        predicted_paras = ht.cut_paragraphs(\"\\n\\n\".join(true_abstract), num_paras=2)\n",
    "        \n",
    "        hyp_bound = []\n",
    "        for sent in true_abstract:\n",
    "            if sent in predicted_paras[0]:\n",
    "                hyp_bound.append(1)\n",
    "            else:\n",
    "                hyp_bound.append(0)\n",
    "        \n",
    "        hyp_conclusion = auto_data[paper_id]['abstract'][hyp_bound.index(0):]\n",
    "        \n",
    "        jaccard_dist_all.append(jaccard_distance(hyp_conclusion, true_conclusion))\n",
    "        \n",
    "        rouge_score = scorer.score(\" \".join(true_conclusion), \" \".join(hyp_conclusion))\n",
    "        \n",
    "        rouge1_fm = rouge_score['rouge1'][2]\n",
    "        rouge2_fm = rouge_score['rouge2'][2]\n",
    "        rougeL_fm = rouge_score['rougeLsum'][2]\n",
    "        \n",
    "        rouge_score_all.append((rouge1_fm + rouge2_fm + rougeL_fm)/3)\n",
    "        \n",
    "        hyp_bound_final = np.zeros(len(auto_data[paper_id]['abstract']))\n",
    "                \n",
    "        for i in range(len(hyp_bound) - 1):\n",
    "            if hyp_bound[i] != hyp_bound[i+1]:\n",
    "                hyp_bound_final[i] = 1\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        if hyp_bound[-1] != hyp_bound[0]:\n",
    "            hyp_bound_final[-1] = 1\n",
    "                                    \n",
    "        hyp_bounds_all[paper_id] = hyp_bound_final    \n",
    "            \n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "for key in ref_bounds_all.keys():\n",
    "    pk_score_all.append(pk(\"\".join(str(int(item)) for item in hyp_bounds_all[key]), \"\".join(str(int(item)) for item in ref_bounds_all[key]), 2))\n",
    "    windiff_score_all.append(windowdiff(\"\".join(str(int(item)) for item in hyp_bounds_all[key]), \"\".join(str(int(item)) for item in ref_bounds_all[key]), 2))\n",
    "\n",
    "print(f\"avg. pk scores for TextTiling: {np.mean(pk_score_all)}\")\n",
    "print(f\"avg. wd scores for TextTiling: {np.mean(windiff_score_all)}\")\n",
    "print(f\"avg. Jaccard index  for TextTiling: {1 - np.mean(jaccard_dist_all)}\")\n",
    "print(f\"avg. Rouge scores for TextTiling: {np.mean(rouge_score_all)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98614aa-884a-4e57-bda8-06b241253aed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026091a7-6ed1-4300-b8cc-baf7ca0ac814",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
